import torch
from torch.utils.data import DataLoader
import torch.optim as optim
import os

from tqdm import tqdm

from base import mylogger, config_toml, current_dir_root
from palm_roi_net.models.loss import CosineSimilarityLoss, PalmCombinedLoss, ClassFiyOneLoss, ClassFiyTwoLoss, \
    CosineMarginOneLoss, CosineMarginTwoLoss
from palm_roi_net.models.restnet_ext import PalmPrintFeatureExtractor
from palm_roi_net.palm_dataset import PalmPrintRandomDataset, data_transforms
from palm_roi_net.utils import save_model, model_utils
from torch.utils.tensorboard import SummaryWriter


def train():
    model_utils.set_seed()
    if (torch.cuda.is_available()):
        device = torch.device(config_toml['TRAIN']['device'])
        torch.backends.cudnn.benchmark = True
        mylogger.warning(f"Deviceï¼š{torch.cuda.get_device_name()}")

    else:
        device = torch.device("cpu")
        mylogger.warning(f"Deviceï¼šOnly Cup...")

    # åˆ›å»º runs exp æ–‡ä»¶
    exp_path = save_model.create_run(0, "vec")
    # æ—¥å¿—ç›¸å…³çš„å‡†å¤‡å·¥ä½œ
    path_board = os.path.join(exp_path, "logs")
    writer = SummaryWriter(path_board)
    save_log_print = os.path.join(exp_path, "log.txt")
    if not os.path.exists(exp_path):
        os.makedirs(exp_path)
    fo = open(file=save_log_print, mode='w', encoding='utf-8')

    # æ„å»ºDataLoder
    train_path = config_toml["TRAIN"]["train_path"]
    train_path = os.path.join(current_dir_root, train_path)
    val_path = config_toml["TRAIN"]["valid_path"]
    val_path = os.path.join(current_dir_root, val_path)

    train_data = PalmPrintRandomDataset(data_dir=train_path,
                                        transform=data_transforms, mode="train"
                                        )

    valid_data = PalmPrintRandomDataset(data_dir=val_path,
                                        transform=data_transforms, mode="val"
                                        )

    mylogger.info(f"the train_data total samples is: {len(train_data)} classes: {len(train_data.classes)}")
    mylogger.info(f"the valid_data total samples is: {len(valid_data)} classes: {len(valid_data.classes)}")

    train_loader = DataLoader(dataset=train_data, batch_size=config_toml['TRAIN']['batch_size'],
                              num_workers=config_toml['TRAIN']['works'], shuffle=config_toml['TRAIN']['shuffle']
                              )

    valid_loader = DataLoader(dataset=valid_data, batch_size=config_toml['TRAIN']['batch_size'])

    # 1.2æ„å»ºç½‘ç»œ
    net = PalmPrintFeatureExtractor(pretrained=True).to(device)
    if config_toml["TRAIN"]["loss"] == 'PalmCombinedLoss':
        combined_loss = PalmCombinedLoss(margin=0.2).to(device)
    elif config_toml["TRAIN"]["loss"] == 'CosineSimilarityLoss':
        combined_loss = CosineSimilarityLoss().to(device)
    elif config_toml["TRAIN"]["loss"] == 'ClassFiyOneLoss':
        combined_loss = ClassFiyOneLoss().to(device)
    elif config_toml["TRAIN"]["loss"] == 'ClassFiyTwoLoss':
        combined_loss = ClassFiyTwoLoss().to(device)
    elif config_toml["TRAIN"]["loss"] == 'CosineMarginOneLoss':
        combined_loss = CosineMarginOneLoss().to(device)
    elif config_toml["TRAIN"]["loss"] == 'CosineMarginTwoLoss':
        combined_loss = CosineMarginTwoLoss().to(device)
    else:
        raise ValueError("loss function is not supported!")
    # 1.3è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.Adam(net.parameters(), lr=config_toml['TRAIN']['lr'])
    # adamè‡ªåŠ¨è°ƒæ•´å­¦ä¹ é€Ÿç‡
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma= 0.5)

    # 2 å¼€å§‹è¿›å…¥è®­ç»ƒæ­¥éª¤
    # 2.1 è¿›å…¥ç½‘ç»œè®­ç»ƒ
    best_weight = None
    total_loss = 0.
    val_loss_total = 0.
    v_time = 0
    best_loss = float("inf")
    for epoch in range(config_toml['TRAIN']['epochs']):
        """
        ä¸‹é¢æ˜¯ä¸€äº›ç”¨æ¥è®°å½•å½“å‰ç½‘ç»œè¿è¡ŒçŠ¶æ€çš„å‚æ•°
        """
        # è®­ç»ƒæŸå¤±
        train_loss = 0
        # éªŒè¯æŸå¤±
        acc_epoch = 0.
        tar_epoch = 0.
        far_epoch = 0.
        frr_epoch = 0.
        trr_epoch = 0.
        roc_auc_epoch = 0.
        net.train()
        mylogger.info("æ­£åœ¨è¿›è¡Œç¬¬{}è½®è®­ç»ƒ".format(epoch + 1))
        for i, (img0, class0, img1, class1, label) in enumerate(
                (tqdm(train_loader, desc=f"Processing the train {epoch+1} epochğŸ˜€"))):
            # forward
            img0, class0, img1, class1, label = img0.to(device), class0.to(device), img1.to(device), class1.to(
                device), label.to(device)

            optimizer.zero_grad()
            # å‰å‘ä¼ æ’­
            feature0 = net(img0)
            feature1 = net(img1)
            # è®¡ç®—æŸå¤±
            # loss,acc_ = cosine_similarity_loss(feature0, feature1, label)
            loss, acc_, tar, far, frr, trr, roc_auc = combined_loss(feature0, class0, feature1, class1, label)
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            # è®°å½•è®­ç»ƒæŸå¤±
            train_loss += loss.item()
            acc_epoch += acc_.item()
            tar_epoch += tar.item()
            far_epoch += far.item()
            frr_epoch += frr.item()
            trr_epoch += trr.item()
            roc_auc_epoch += roc_auc.item()
            # æ›´æ–°å­¦ä¹ ç‡
            # scheduler.step()
            # æ˜¾ç¤ºlogçš„æŸå¤±
            if (i + 1) % config_toml['TRAIN']['log_interval'] == 0:
                # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆä¸€ä¸ªbatchçš„ï¼‰
                log_loss_mean_train = train_loss / (i + 1)
                info = "è®­ç»ƒ:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc: {:.4f}" \
                    .format \
                        (
                        epoch, config_toml['TRAIN']['epochs'], i + 1,
                        len(train_loader), log_loss_mean_train, acc_epoch / (i + 1)

                    )
                print(info, file=fo)
                mylogger.info(info)

        train_loss /= len(train_loader)
        acc_epoch /= len(train_loader)
        tar_epoch /= len(train_loader)
        far_epoch /= len(train_loader)
        frr_epoch /= len(train_loader)
        trr_epoch /= len(train_loader)
        roc_auc_epoch /= len(train_loader)
        # æ€»ä½“æŸå¤±
        total_loss += train_loss
        # tensorboard ç»˜å›¾
        # æ€»ä½“æŸå¤±å€¼æ˜¯ä¸Šæ›²çº¿
        # æ¯è½®æŸå¤±å€¼æ˜¯ä¸‹æ›²çº¿
        writer.add_scalar("è®­ç»ƒæ€»ä½“æŸå¤±å€¼", total_loss, epoch)
        writer.add_scalar("è®­ç»ƒæ¯è½®æŸå¤±å€¼", train_loss, epoch)
        writer.add_scalar("è®­ç»ƒå‡†ç¡®ç‡", acc_epoch, epoch)
        writer.add_scalar("è®­ç»ƒTAR", tar_epoch, epoch)
        writer.add_scalar("è®­ç»ƒFAR", far_epoch, epoch)
        writer.add_scalar("è®­ç»ƒFRR", frr_epoch, epoch)
        writer.add_scalar("è®­ç»ƒTRR", trr_epoch, epoch)
        writer.add_scalar("è®­ç»ƒROC_AUC", roc_auc_epoch, epoch)

        # ä¿å­˜æŸå¤±æœ€å°çš„
        if (train_loss < best_loss):
            best_weight = net.state_dict()
            best_loss = train_loss

        # 2.2 è¿›å…¥éªŒè¯èŠ‚ç‚¹

        if (epoch + 1) % config_toml["TRAIN"]["val_interval"] == 0:
            """
            è¿™éƒ¨åˆ†å’Œè®­ç»ƒçš„é‚£éƒ¨åˆ†æ˜¯ç±»ä¼¼çš„ï¼Œå¯ä»¥å¿½ç•¥è¿™éƒ¨åˆ†çš„ä»£ç 
            """
            val_loss = 0.
            val_acc_time = 0.
            val_tar_time = 0.
            val_far_time = 0.
            val_frr_time = 0.
            val_trr_time = 0.
            val_roc_auc_time = 0.
            net.eval()
            with torch.no_grad():
                for j, (img0, class0, img1, class1, label) in enumerate((tqdm(valid_loader, desc="Processing the valid one epochğŸ˜€"))):

                    img0, class0, img1, class1, label = img0.to(device), class0.to(device), img1.to(device), class1.to(
                        device), label.to(device)

                    # å‰å‘ä¼ æ’­
                    feature0 = net(img0)
                    feature1 = net(img1)
                    # è®¡ç®—æŸå¤±
                    # loss,val_acc = cosine_similarity_loss(feature0, feature1, label)
                    loss, val_acc, val_tar, val_far, val_frr, val_trr, val_roc_auc = combined_loss(feature0, class0, feature1, class1, label)

                    val_loss += loss.item()
                    val_acc_time += val_acc.item()
                    val_tar_time += val_tar.item()
                    val_far_time += val_far.item()
                    val_frr_time += val_frr.item()
                    val_trr_time += val_trr.item()
                    val_roc_auc_time += val_roc_auc.item()

                info_val = "æµ‹è¯•:\tEpoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc: {:.4f}".format \
                        (
                        epoch, config_toml["TRAIN"]["epochs"], (j + 1),
                        len(valid_loader), val_loss/(j+1), val_acc_time / (j + 1)
                    )
                mylogger.info(info_val)

                val_loss /= len(valid_loader)
                val_acc_time /= len(valid_loader)
                val_tar_time /= len(valid_loader)
                val_far_time /= len(valid_loader)
                val_frr_time /= len(valid_loader)
                val_trr_time /= len(valid_loader)
                val_roc_auc_time /= len(valid_loader)
                print(info_val, file=fo)
                val_loss_total += val_loss

                writer.add_scalar("æµ‹è¯•æ€»ä½“æŸå¤±å€¼", val_loss_total, v_time)
                writer.add_scalar("æµ‹è¯•æ¯è½®æŸå¤±å€¼", val_loss, v_time)
                writer.add_scalar("æµ‹è¯•å‡†ç¡®ç‡", val_acc_time, v_time)
                writer.add_scalar("æµ‹è¯•TAR", val_tar_time, v_time)
                writer.add_scalar("æµ‹è¯•FAR", val_far_time, v_time)
                writer.add_scalar("æµ‹è¯•FRR", val_frr_time, v_time)
                writer.add_scalar("æµ‹è¯•TRR", val_trr_time, v_time)
                writer.add_scalar("æµ‹è¯•ROC_AUC", val_roc_auc_time, v_time)
                v_time += 1

        if (epoch + 1) % config_toml["TRAIN"]["save_epoch"] == 0:
            # ä¿å­˜æ¨¡å‹
            save_model.save_model(exp_path, best_weight, net.state_dict(), index=epoch + 1)
    # æœ€åä¸€æ¬¡çš„æƒé‡
    last_weight = net.state_dict()
    # ä¿å­˜æ¨¡å‹
    save_model.save_model(exp_path, best_weight, last_weight,index=config_toml['TRAIN']['epochs'])
    fo.close()
    mylogger.info(f"tensorboard dir is:{path_board}")
    writer.close()


if __name__ == '__main__':
    train()
    # nohup python3.10 train_palm_ext.py  >> log.txt 2>&1 &
    # tensorboard --logdir=runs/train_vec/ --port=6006 --host=0.0.0.0
