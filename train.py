import torch
from torch.utils.data import DataLoader
import torch.optim as optim
import os

from tqdm import tqdm

from base import mylogger, config_toml, current_dir_root
from palm_roi_net.models.loss import CosineSimilarityLoss
from palm_roi_net.models.restnet_ext import PalmPrintFeatureExtractor
from palm_roi_net.palm_dataset import PalmPrintStaticDataset, data_transforms, PalmPrintDynamicDataset
from palm_roi_net.utils import save_model, model_utils
from palm_roi_net.utils import log
from torch.utils.tensorboard import SummaryWriter


def train():
    model_utils.set_seed()
    if (torch.cuda.is_available()):
        device = torch.device(config_toml['TRAIN']['device'])
        torch.backends.cudnn.benchmark = True
        mylogger.warning(f"Deviceï¼š{torch.cuda.get_device_name()}")

    else:
        device = torch.device("cpu")
        mylogger.warning(f"Deviceï¼šOnly Cup...")

    # åˆ›å»º runs exp æ–‡ä»¶
    exp_path = save_model.create_run(0, "vec")
    # æ—¥å¿—ç›¸å…³çš„å‡†å¤‡å·¥ä½œ
    path_board = os.path.join(exp_path, "logs")
    writer = SummaryWriter(path_board)
    save_log_print = os.path.join(exp_path, "log.txt")
    if not os.path.exists(exp_path):
        os.makedirs(exp_path)
    fo = open(file=save_log_print, mode='w', encoding='utf-8')

    # æ„å»ºDataLoder
    train_path = config_toml["TRAIN"]["train_path"]
    train_path = os.path.join(current_dir_root, train_path)
    val_path = config_toml["TRAIN"]["valid_path"]
    val_path = os.path.join(current_dir_root, val_path)

    # è®­ç»ƒä½¿ç”¨åŠ¨æ€ç”Ÿæˆ
    train_data = PalmPrintDynamicDataset(data_dir=train_path,
                                         transform=data_transforms, mode="train"
                                         )
    # éªŒè¯ä½¿ç”¨é™æ€ç”Ÿæˆ
    valid_data = PalmPrintStaticDataset(data_dir=val_path,
                                        transform=data_transforms, mode="train"
                                        )
    mylogger.info(f"the valid_data total samples is: {len(valid_data)}")
    train_loader = DataLoader(dataset=train_data, batch_size=config_toml['TRAIN']['batch_size'],
                              num_workers=config_toml['TRAIN']['works'], shuffle=config_toml['TRAIN']['shuffle']
                              )
    mylogger.info(f"the train_data total samples is: {len(train_data)}")
    valid_loader = DataLoader(dataset=valid_data, batch_size=config_toml['TRAIN']['batch_size'])

    # 1.2æ„å»ºç½‘ç»œ
    net = PalmPrintFeatureExtractor(pretrained=True).to(device)
    cosine_similarity_loss = CosineSimilarityLoss(margin=0.2).to(device)

    # 1.3è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.Adam(net.parameters(), lr=config_toml['TRAIN']['lr'])
    # è®¾ç½®å­¦ä¹ ç‡ä¸‹é™ç­–ç•¥,é»˜è®¤çš„ä¹Ÿå¯ä»¥ï¼Œé‚£å°±ä¸è®¾ç½®å˜›ï¼Œä¸»è¦æ˜¯ä¸æ–­å»è‡ªåŠ¨è°ƒæ•´å­¦ä¹ çš„é‚£ä¸ªé€Ÿåº¦
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.01)

    # 2 å¼€å§‹è¿›å…¥è®­ç»ƒæ­¥éª¤
    # 2.1 è¿›å…¥ç½‘ç»œè®­ç»ƒ
    best_weight = None
    total_loss = 0.
    val_loss_total = 0.
    v_time = 0
    best_loss = float("inf")
    for epoch in range(config_toml['TRAIN']['epochs']):
        """
        ä¸‹é¢æ˜¯ä¸€äº›ç”¨æ¥è®°å½•å½“å‰ç½‘ç»œè¿è¡ŒçŠ¶æ€çš„å‚æ•°
        """
        # è®­ç»ƒæŸå¤±
        train_loss = 0
        # éªŒè¯æŸå¤±
        val_loss = 0
        net.train()
        mylogger.info("æ­£åœ¨è¿›è¡Œç¬¬{}è½®è®­ç»ƒ".format(epoch + 1))
        for i, (img0, img1, label) in enumerate((tqdm(train_loader, desc="Processing the train one epochğŸ˜€"))):
            # forward
            optimizer.zero_grad()
            img0, img1, label = img0.to(device), img1.to(device), label.to(device)
            # å‰å‘ä¼ æ’­
            feature0 = net(img0)
            feature1 = net(img1)
            # è®¡ç®—æŸå¤±
            loss = cosine_similarity_loss(feature0, feature1, label)
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            # è®°å½•è®­ç»ƒæŸå¤±
            train_loss += loss.item()
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            # æ˜¾ç¤ºlogçš„æŸå¤±
            if (i + 1) % config_toml['TRAIN']['log_interval'] == 0:
                # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆä¸€ä¸ªbatchçš„ï¼‰
                log_loss_mean_train = train_loss / (i + 1)
                info = "è®­ç»ƒ:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}" \
                    .format \
                        (
                        epoch, config_toml['TRAIN']['epochs'], i + 1, len(train_loader), log_loss_mean_train
                    )
                print(info, file=fo)
                mylogger.info(info)
        # æ€»ä½“æŸå¤±
        total_loss += train_loss
        # tensorboard ç»˜å›¾
        # æ€»ä½“æŸå¤±å€¼æ˜¯ä¸Šæ›²çº¿
        # æ¯è½®æŸå¤±å€¼æ˜¯ä¸‹æ›²çº¿
        writer.add_scalar("æ€»ä½“æŸå¤±å€¼", total_loss, epoch)
        writer.add_scalar("æ¯è½®æŸå¤±å€¼", train_loss, epoch)

        # ä¿å­˜æ•ˆæœæœ€å¥½çš„ç©æ„
        if (train_loss < best_loss):
            best_weight = net.state_dict()
            best_loss = train_loss

        # 2.2 è¿›å…¥éªŒè¯èŠ‚ç‚¹

        if (epoch + 1) % config_toml["TRAIN"]["val_interval"] == 0:
            """
            è¿™éƒ¨åˆ†å’Œè®­ç»ƒçš„é‚£éƒ¨åˆ†æ˜¯ç±»ä¼¼çš„ï¼Œå¯ä»¥å¿½ç•¥è¿™éƒ¨åˆ†çš„ä»£ç 
            """
            net.eval()
            with torch.no_grad():
                for j, img0, img1, label in enumerate((tqdm(valid_loader, desc="Processing the valid one epochğŸ˜€"))):
                    img0, img1, label = img0.to(device), img1.to(device), label.to(device)
                    # å‰å‘ä¼ æ’­
                    feature0 = net(img0)
                    feature1 = net(img1)
                    # è®¡ç®—æŸå¤±
                    loss = cosine_similarity_loss(feature0, feature1, label)

                    val_loss += loss.item()

                info_val = "æµ‹è¯•:\tEpoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} ".format \
                        (
                        epoch, config_toml["TRAIN"]["epochs"], (j + 1),
                        len(valid_loader), val_loss
                    )
                mylogger.info(info_val)
                print(info_val, file=fo)
                val_loss_total += val_loss

                writer.add_scalar("æµ‹è¯•æ€»ä½“æŸå¤±", val_loss, v_time)
                writer.add_scalar("æ¯æ¬¡æµ‹è¯•æ€»æŸå¤±æ€»å€¼", val_loss_total, v_time)
                v_time += 1

    # æœ€åä¸€æ¬¡çš„æƒé‡
    last_weight = net.state_dict()
    # ä¿å­˜æ¨¡å‹
    save_model.save_model(exp_path, best_weight, last_weight)
    fo.close()
    mylogger.info("tensorboard dir is:", path_board)
    writer.close()


if __name__ == '__main__':
    train()
    # tensorboard --logdir=runs/traindetect/epx0/logs
